{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0639ae59",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fzimmermann89/mr2/blob/main/examples/notebooks/apply_pinqi.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103fbbf",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec('mr2'):\n",
    "    %pip install mrtwo[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d1e46",
   "metadata": {},
   "source": [
    "# End-to-end physics informed network for quantitative MRI (PINQI)\n",
    "A recent DL approach, PINQI, approaches learned quantitative MRI by half quadratic splitting to alternate between two\n",
    "subproblems. The first is a linear image reconstruction task\n",
    "$$\n",
    "\\underset{\\mathbf{x}}{\\min} \\frac{1}{2} \\| \\mathbf{A} \\mathbf{x} - \\mathbf{y} \\|_2^2\n",
    "+ \\frac{\\lambda_\\mathbf{x}}{2} \\left\\| \\mathbf{x} - \\mathbf{x}_{\\text{reg}} \\right\\|_2^2\n",
    "+ \\frac{\\lambda_{\\mathbf{q}}}{2} \\left\\| \\mathbf{q}(\\mathbf{p}) - \\mathbf{x} \\right\\|_2^2\n",
    "$$\n",
    "with $\\mathbf{x}$ being intermediary qualitative images, $\\lambda_{\\mathbf{x}}$ and $\\lambda_{\\mathbf{q}}$ being\n",
    "regularization strengths and $\\mathbf{x}_{\\text{reg}}$ denoting an image prior for regularization.\n",
    "The second, non-linear, subproblem is finding the quantitative parameters by solving\n",
    "$$\n",
    "\\underset{\\mathbf{p}}{\\min} \\frac{\\lambda_{\\mathbf{q}}}{2}\\left \\| \\mathbf{q}(\\vec{p}) - \\mathbf{x} \\right\\|_2^2\n",
    "+ \\frac{\\lambda_{\\mathbf{p}}}{2} \\left\\| \\mathbf{p} - \\mathbf{p}_{\\text{reg}} \\right\\|_2^2.\n",
    "$$\n",
    "Here, $\\mathbf{p}_{\\text{reg}}$ is a prior on the parameter maps and $\\lambda_{\\mathbf{p}}$ the associated weight for\n",
    "regularization.\n",
    "In PINQI, a solution is found by iterating between both subproblems. In each iteration $k=1,\\ldots,T$,\n",
    "the image and parameter priors are updated by U-Nets. The network parameters and the regularization strengths\n",
    "are trained end-to-end.\n",
    "Here, we apply a trained PINQI model to a validation set. We first define the dataset, then define the PINQI model,\n",
    "before loading the model weights and applying it to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99b8b9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We base the dataset on the BrainWeb phantom (`mr2.phantoms.brainweb.BrainwebSlices`) and simulate Cartesian random\n",
    "undersampling in phase encode direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3861081",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Literal, TypedDict\n",
    "\n",
    "import einops\n",
    "import mr2\n",
    "import torch\n",
    "\n",
    "# mr2.phantoms.brainweb.download_brainweb(workers=2, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d6fca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class BatchType(TypedDict):\n",
    "    \"\"\"Typehint for a batch of data.\"\"\"\n",
    "\n",
    "    kdata: mr2.data.KData\n",
    "    csm: mr2.data.CsmData\n",
    "    m0: torch.Tensor\n",
    "    t1: torch.Tensor\n",
    "    mask: torch.Tensor\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset[BatchType]):\n",
    "    \"\"\"A brainweb based cartesian qMRI dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder: Path,\n",
    "        signalmodel: mr2.operators.SignalModel,\n",
    "        n_images: int,\n",
    "        size: int,\n",
    "        acceleration: int,\n",
    "        n_coils: int,\n",
    "        max_noise: float,\n",
    "        orientation: Sequence[Literal['axial', 'coronal', 'sagittal']],\n",
    "        random: bool = True,\n",
    "    ):\n",
    "        \"\"\"Initialize the dataset.\"\"\"\n",
    "        if random:\n",
    "            augment = mr2.phantoms.brainweb.augment(size=size)\n",
    "        else:\n",
    "            augment = mr2.phantoms.brainweb.augment(\n",
    "                size=size,\n",
    "                max_random_shear=0,\n",
    "                max_random_rotation=0,\n",
    "                max_random_scaling_factor=0,\n",
    "                p_horizontal_flip=0,\n",
    "                p_vertical_flip=1.0,\n",
    "            )\n",
    "        self.phantom = mr2.phantoms.brainweb.BrainwebSlices(\n",
    "            folder=folder,\n",
    "            what=('m0', 't1', 'mask'),\n",
    "            seed='index' if not random else 'random',\n",
    "            slice_preparation=augment,\n",
    "            orientation=orientation,\n",
    "        )\n",
    "        self.signalmodel = deepcopy(signalmodel)\n",
    "        self.encoding_matrix = mr2.data.SpatialDimension(1, size, size)\n",
    "        self.fov = mr2.data.SpatialDimension(0.01, 0.25, 0.25)\n",
    "        self.acceleration = acceleration\n",
    "        self.n_coils = n_coils\n",
    "        self._random = random\n",
    "        self.max_noise = max_noise\n",
    "        self._n_images = n_images\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get the length of the dataset.\"\"\"\n",
    "        return len(self.phantom)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"Get an item from the dataset.\"\"\"\n",
    "        phantom = self.phantom[index]\n",
    "        (images,) = self.signalmodel(phantom['m0'], phantom['t1'])\n",
    "        seed = int(torch.randint(0, 1000000, (1,))) if self._random else index\n",
    "\n",
    "        traj = mr2.data.traj_calculators.KTrajectoryCartesian.gaussian_variable_density(\n",
    "            encoding_matrix=self.encoding_matrix,\n",
    "            seed=seed,\n",
    "            acceleration=self.acceleration,\n",
    "            fwhm_ratio=1.5,\n",
    "            n_center=12,\n",
    "            n_other=(self._n_images,),\n",
    "        )\n",
    "        header = mr2.data.KHeader(\n",
    "            encoding_matrix=self.encoding_matrix,\n",
    "            recon_matrix=self.encoding_matrix,\n",
    "            recon_fov=self.fov,\n",
    "            encoding_fov=self.fov,\n",
    "        )\n",
    "\n",
    "        if isinstance(self.signalmodel, mr2.operators.models.SaturationRecovery):\n",
    "            header.ti = self.signalmodel.saturation_time.tolist()\n",
    "        elif isinstance(self.signalmodel, mr2.operators.models.InversionRecovery):\n",
    "            header.ti = self.signalmodel.ti.tolist()\n",
    "\n",
    "        fourier_op = mr2.operators.FourierOp(self.encoding_matrix, self.encoding_matrix, traj)\n",
    "        if self.n_coils > 1:\n",
    "            csm_tensor = mr2.phantoms.coils.birdcage_2d(self.n_coils, self.encoding_matrix)\n",
    "        else:\n",
    "            csm_tensor = torch.ones(1, 1, *self.encoding_matrix.zyx)\n",
    "        csm = mr2.data.CsmData(csm_tensor, header)\n",
    "        images = einops.rearrange(images, 't y x -> t 1 1 y x')\n",
    "        (data,) = (fourier_op @ csm.as_operator())(images)\n",
    "        data = data + torch.randn_like(data) * torch.rand(1) * self.max_noise * data.std()\n",
    "        kdata = mr2.data.KData(header, data, traj)\n",
    "        return {'kdata': kdata, 'csm': csm, **phantom}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d365b087",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## PINQI\n",
    "Next, We define the PINQI model. Here we can make use of the diffferntiable optimization operators in MRtwo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94600e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINQI(torch.nn.Module):\n",
    "    \"\"\"PINQI model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        signalmodel: mr2.operators.SignalModel,\n",
    "        constraints_op: mr2.operators.ConstraintsOp | mr2.operators.MultiIdentityOp,\n",
    "        parameter_is_complex: Sequence[bool],\n",
    "        n_images: int,\n",
    "        n_iterations: int,\n",
    "        n_features_parameter_net: Sequence[int],\n",
    "        n_features_image_net: Sequence[int],\n",
    "    ):\n",
    "        \"\"\"Initialize the PINQI model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.signalmodel = mr2.operators.RearrangeOp('t batch ... -> batch t ...') @ signalmodel @ constraints_op\n",
    "        self.constraints_op = constraints_op\n",
    "        self._n_images = n_images\n",
    "        self._parameter_is_complex = parameter_is_complex\n",
    "        real_parameters = sum(1 for c in parameter_is_complex if c) + len(parameter_is_complex)\n",
    "        self.parameter_net = mr2.nn.nets.UNet(\n",
    "            n_dim=2,\n",
    "            n_channels_in=n_images * 2,\n",
    "            n_channels_out=real_parameters,\n",
    "            attention_depths=(-1, -2),\n",
    "            n_features=n_features_parameter_net,\n",
    "            cond_dim=128,\n",
    "        )\n",
    "\n",
    "        self.image_net = mr2.nn.nets.UNet(\n",
    "            n_dim=2,\n",
    "            n_channels_in=2,\n",
    "            n_channels_out=2,\n",
    "            attention_depths=(),\n",
    "            n_features=n_features_image_net,\n",
    "            cond_dim=128,\n",
    "        )\n",
    "        self.lambdas_raw = torch.nn.Parameter(torch.ones(n_iterations, 3))\n",
    "        self.softplus = torch.nn.Softplus(beta=5)\n",
    "        self.iteration_embedding = torch.nn.Embedding(n_iterations + 1, 128)\n",
    "\n",
    "        def objective_factory(\n",
    "            lambda_parameters: torch.Tensor,\n",
    "            image: torch.Tensor,\n",
    "            *parameter_reg: torch.Tensor,\n",
    "        ) -> mr2.operators.Operator:\n",
    "            dc = mr2.operators.functionals.L2NormSquared(image) @ self.signalmodel\n",
    "            reg = mr2.operators.ProximableFunctionalSeparableSum(\n",
    "                *[mr2.operators.functionals.L2NormSquared(r) for r in parameter_reg]\n",
    "            )\n",
    "            return dc + lambda_parameters * reg\n",
    "\n",
    "        def initializer(_l: torch.Tensor, _i: torch.Tensor, *parameter_reg: torch.Tensor) -> tuple[torch.Tensor, ...]:\n",
    "            return parameter_reg\n",
    "\n",
    "        self.nonlinear_solver = mr2.operators.OptimizerOp(objective_factory, initializer)\n",
    "        # This can be done once, as the signal model is the same for all samples.\n",
    "\n",
    "    def get_linear_solver(self, gram: mr2.operators.LinearOperator) -> mr2.operators.ConjugateGradientOp:\n",
    "        \"\"\"Set up the linear solver.\"\"\"\n",
    "        # This needs to be done for each sample, as the undersampling pattern and csm are different for each sample,\n",
    "        # thus the gram operator of the acquisition operator is different for each sample.\n",
    "\n",
    "        def operator_factory(\n",
    "            lambda_image: torch.Tensor,\n",
    "            lambda_q: torch.Tensor,\n",
    "            *_,\n",
    "        ):\n",
    "            return gram + lambda_image + lambda_q\n",
    "\n",
    "        def rhs_factory(\n",
    "            lambda_image: torch.Tensor,\n",
    "            lambda_q: torch.Tensor,\n",
    "            image_reg: torch.Tensor,\n",
    "            signal: torch.Tensor,\n",
    "            zero_filled_image: torch.Tensor,\n",
    "        ):\n",
    "            return (zero_filled_image + lambda_image * image_reg + lambda_q * signal,)\n",
    "\n",
    "        return mr2.operators.ConjugateGradientOp(\n",
    "            operator_factory=operator_factory,\n",
    "            rhs_factory=rhs_factory,\n",
    "        )\n",
    "\n",
    "    def get_parameter_reg(self, image: torch.Tensor, iteration: int = 0) -> tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Get the parameter regularization.\"\"\"\n",
    "        image = einops.rearrange(\n",
    "            torch.view_as_real(image),\n",
    "            'batch t 1 1 y x complex-> batch (t complex) y x',\n",
    "        )\n",
    "        cond = self.iteration_embedding(torch.tensor(iteration, device=image.device))[None]\n",
    "        parameters = self.parameter_net(image.contiguous(), cond=cond)\n",
    "        parameters = einops.rearrange(parameters, 'batch parameters y x-> parameters batch 1 1 y x')\n",
    "        i = 0\n",
    "        result = []\n",
    "        for is_complex in self._parameter_is_complex:\n",
    "            if is_complex:\n",
    "                result.append(torch.complex(parameters[i], parameters[i + 1]))\n",
    "                i += 2\n",
    "            else:\n",
    "                result.append(parameters[i])\n",
    "                i += 1\n",
    "        return tuple(result)\n",
    "\n",
    "    def get_image_reg(self, image: torch.Tensor, iteration: int = 0) -> torch.Tensor:\n",
    "        \"\"\"Get the image regularization.\"\"\"\n",
    "        batch = image.shape[0]\n",
    "        image = einops.rearrange(\n",
    "            torch.view_as_real(image),\n",
    "            'batch t 1 1 y x complex-> (batch t) complex y x',\n",
    "        )\n",
    "        cond = self.iteration_embedding(torch.tensor(iteration, device=image.device))[None]\n",
    "        image = image + self.image_net(image.contiguous(), cond=cond)\n",
    "        image = einops.rearrange(image, '(batch t) complex y x-> batch t 1 1 y x complex', batch=batch)\n",
    "        return torch.view_as_complex(image.contiguous())\n",
    "\n",
    "    def forward(self, kdata: mr2.data.KData, csm: mr2.data.CsmData) -> tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Estimate the quantitative parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        kdata\n",
    "            The k-space data.\n",
    "        csm\n",
    "            The coil sensitivity maps.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        parameters\n",
    "            The quantitative parameters (tuple of parameter tensors).\n",
    "        \"\"\"\n",
    "        csm_op = csm.as_operator()\n",
    "        fourier_op = mr2.operators.FourierOp.from_kdata(kdata)\n",
    "        acquisition_op = fourier_op @ csm_op\n",
    "        gram = acquisition_op.gram\n",
    "        (zero_filled_image,) = acquisition_op.H(kdata.data)\n",
    "        (images,) = mr2.algorithms.optimizers.cg(gram, zero_filled_image, max_iterations=2)\n",
    "        parameters = self.get_parameter_reg(images, 0)\n",
    "        linear_solver = self.get_linear_solver(gram)\n",
    "\n",
    "        for i, (lambda_image, lambda_q, lambda_parameter) in enumerate(self.softplus(self.lambdas_raw)):\n",
    "            # linear subproblem 1\n",
    "            image_reg = self.get_image_reg(images, i + 1)\n",
    "            (signal,) = self.signalmodel(*parameters)\n",
    "            (images,) = linear_solver(lambda_image, lambda_q, image_reg, signal, zero_filled_image)\n",
    "            # nonlinear subproblem 2\n",
    "            parameters_reg = self.get_parameter_reg(images, i + 1)\n",
    "            parameters = self.nonlinear_solver(lambda_parameter, images, *parameters_reg)\n",
    "        if self.constraints_op is not None:\n",
    "            # map the parameters into the constrained space\n",
    "            parameters = self.constraints_op(*parameters)\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69570c58",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# As a baseline methods for comparison, we use a simple non-learned approach. We reconstruct the qualitative images at\n",
    "# different saturation times using iterative SENSE. We then perform a  constrained non-linear least squares regression\n",
    "# using L-BFGS to obtain the parameter maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d81740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_solution(\n",
    "    signalmodel: mr2.operators.SignalModel,\n",
    "    constraints_op: mr2.operators.ConstraintsOp | mr2.operators.MultiIdentityOp,\n",
    "    parameter_is_complex: Sequence[bool],\n",
    "    kdata: mr2.data.KData,\n",
    "    csm: mr2.data.CsmData,\n",
    ") -> tuple[torch.Tensor, ...]:\n",
    "    \"\"\"Compute a baseline solution using SENSE + Regression.\"\"\"\n",
    "    sense = mr2.algorithms.reconstruction.IterativeSENSEReconstruction(kdata, csm=csm)\n",
    "    images = sense(kdata)\n",
    "    objective = mr2.operators.functionals.L2NormSquared(images.data) @ signalmodel @ constraints_op\n",
    "    initial_values = tuple(\n",
    "        torch.zeros(\n",
    "            images.shape[1:],\n",
    "            device=images.device,\n",
    "            dtype=torch.complex64 if is_complex else torch.float32,\n",
    "        )\n",
    "        for is_complex in parameter_is_complex\n",
    "    )\n",
    "    solution = constraints_op(*mr2.algorithms.optimizers.lbfgs(objective, initial_values))\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237117b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_folder = Path('/home/zimmer08/.cache/mr2/brainweb')\n",
    "\n",
    "signalmodel = mr2.operators.models.SaturationRecovery((0.5, 1.0, 1.5, 2.0, 8.0))\n",
    "constraints_op = mr2.operators.ConstraintsOp(\n",
    "    bounds=(\n",
    "        (-2, 2),  # M0 in [-2, 2]\n",
    "        (0.01, 6.0),  # T1 is constrained between 10 ms and 6 s\n",
    "    )\n",
    ")\n",
    "n_images = len(signalmodel.saturation_time)\n",
    "parameter_is_complex = [True, False]\n",
    "\n",
    "\n",
    "dataset = torch.utils.data.Subset(\n",
    "    Dataset(\n",
    "        folder=data_folder,\n",
    "        signalmodel=signalmodel,\n",
    "        n_images=n_images,\n",
    "        size=192,\n",
    "        acceleration=8,\n",
    "        n_coils=8,\n",
    "        max_noise=0.05,\n",
    "        orientation=('axial',),\n",
    "        random=False,\n",
    "    ),\n",
    "    list(range(500)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1679b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./examples/scripts/last.ckpt', map_location='cpu')\n",
    "hyper_parameters = checkpoint['hyper_parameters']\n",
    "\n",
    "\n",
    "pinqi = PINQI(\n",
    "    signalmodel=signalmodel,\n",
    "    constraints_op=constraints_op,\n",
    "    parameter_is_complex=parameter_is_complex,\n",
    "    n_images=n_images,\n",
    "    n_iterations=hyper_parameters['n_iterations'],\n",
    "    n_features_parameter_net=hyper_parameters['n_features_parameter_net'],\n",
    "    n_features_image_net=hyper_parameters['n_features_image_net'],\n",
    ")\n",
    "state_dict = {\n",
    "    k.replace('pinqi.', '').replace('_orig_mod.', ''): v\n",
    "    for k, v in checkpoint['state_dict'].items()\n",
    "    if 'baseline' not in k\n",
    "}\n",
    "pinqi.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c971d44",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "batch = dataset[40]\n",
    "csm, kdata = batch['csm'], batch['kdata']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pinqi, csm, kdata = pinqi.cuda(), csm.cuda(), kdata.cuda()\n",
    "parameters = pinqi(kdata[None], csm[None])\n",
    "with torch.no_grad():\n",
    "    predicted_m0, predicted_t1 = (p.cpu().detach().squeeze() for p in parameters)\n",
    "baseline_m0, baseline_t1 = baseline_solution(signalmodel, constraints_op, parameter_is_complex, kdata, csm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da6480",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "(ssim_t1,) = mr2.operators.functionals.SSIM(batch['t1'][None], batch['mask'][None])(predicted_t1[None])\n",
    "(mse_t1,) = mr2.operators.functionals.MSE(batch['t1'], batch['mask'])(predicted_t1)\n",
    "\n",
    "(mse_baseline,) = mr2.operators.functionals.MSE(batch['t1'], batch['mask'])(baseline_t1)\n",
    "nrmse_t1 = torch.sqrt(mse_t1) / batch['t1'][batch['mask']].max()\n",
    "(ssim_baseline,) = mr2.operators.functionals.SSIM(batch['t1'][None], batch['mask'][None])(baseline_t1[None])\n",
    "nrmse_baseline = torch.sqrt(mse_baseline) / batch['t1'][batch['mask']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from cmap import Colormap\n",
    "\n",
    "cmap = Colormap('lipari').to_matplotlib()\n",
    "\n",
    "print(f'SSIM: {ssim_baseline.item():.4f}, NRMSE: {nrmse_baseline.item():.4f}')\n",
    "print(f'SSIM: {ssim_t1.item():.4f}, NRMSE: {nrmse_t1.item():.4f}')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    1,\n",
    "    5,\n",
    "    gridspec_kw={\n",
    "        'width_ratios': [1, 1, 1, 0.28, 0.075],\n",
    "        'wspace': -0.25,\n",
    "    },\n",
    "    figsize=(6.5, 2.5),\n",
    ")\n",
    "baseline_t1 = baseline_t1.squeeze()\n",
    "baseline_t1[~batch['mask']] = torch.nan\n",
    "ax[0].imshow(baseline_t1, vmin=0, vmax=2, cmap=cmap)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('SENSE + NLS')\n",
    "ax[0].text(\n",
    "    0.5,\n",
    "    -0.00,\n",
    "    f'SSIM: {ssim_baseline.item():.2f}',\n",
    "    color='black',\n",
    "    horizontalalignment='center',\n",
    "    verticalalignment='top',\n",
    "    transform=ax[0].transAxes,\n",
    "    size=11,\n",
    ")\n",
    "predicted_t1 = predicted_t1.squeeze()\n",
    "predicted_t1[~batch['mask']] = torch.nan\n",
    "ax[1].imshow(predicted_t1, vmin=0, vmax=2, cmap=cmap)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('PINQI')\n",
    "ax[1].text(\n",
    "    0.5,\n",
    "    -0.0,\n",
    "    f'SSIM: {ssim_t1.item():.2f}',\n",
    "    color='black',\n",
    "    horizontalalignment='center',\n",
    "    verticalalignment='top',\n",
    "    transform=ax[1].transAxes,\n",
    "    size=11,\n",
    ")\n",
    "\n",
    "target_t1 = batch['t1'].squeeze()\n",
    "target_t1[~batch['mask']] = torch.nan\n",
    "im = ax[2].imshow(target_t1, vmin=0, vmax=2, cmap=cmap)\n",
    "ax[2].axis('off')\n",
    "ax[2].set_title(\n",
    "    'Ground Truth',\n",
    ")\n",
    "ax[-2].axis('off')\n",
    "fig.tight_layout()\n",
    "plt.colorbar(im, cax=ax[-1], label='$T_1$ (s)')\n",
    "fig.savefig(\n",
    "    '/home/zimmer08/code/mr2/examples/scripts/pinqi_t1_3.pdf',\n",
    "    bbox_inches='tight',\n",
    "    pad_inches=0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "mystnb,tags,-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
